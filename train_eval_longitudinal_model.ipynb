{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7cc382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magda/.local/share/virtualenvs/classification-FCcuBugl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from load_data import *\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import *\n",
    "from models import *\n",
    "from train_test_utils import *\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda2e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_with_seed = 1964 #random.randint(0, 4294967295)\n",
    "torch.manual_seed(run_with_seed)\n",
    "np.random.seed(run_with_seed)\n",
    "torch.cuda.manual_seed(run_with_seed)\n",
    "\n",
    "# Load data\n",
    "path = 'full_per_visit_data_2021-03-26_processed.csv'\n",
    "\n",
    "# Path to save the processed version of the dataset, if needed\n",
    "w_path = ''\n",
    "\n",
    "input_feats = load_longitudinal_tabular_data(input_path=path, write_path=w_path, quick=True, write_csv=False)\n",
    "\n",
    "# Because this variable should not be used\n",
    "input_feats = input_feats.drop(columns=['aces_total'])\n",
    "\n",
    "# We are predicting age and construct in a multi-task setting\n",
    "age = False\n",
    "\n",
    "# seq2seq dictates if we predict all time-steps or only the last one\n",
    "seq2seq = False\n",
    "\n",
    "# Options are 'negative_valence' and 'positive_valence'\n",
    "construct = 'negative_valence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66dcc449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'no construct at all' label separate the subjects to diseased and control so that\n",
    "# we can create stratified splits\n",
    "\n",
    "labels = {}\n",
    "labels_control_diseased = {}\n",
    "ages = {}\n",
    "for key in list(np.unique(input_feats.loc[:, 'subject'])):\n",
    "    subj_v = input_feats[input_feats['subject'] == key]\n",
    "    labels[key] = subj_v.loc[:, construct]\n",
    "    ages[key] = subj_v.loc[:, 'visit_age']\n",
    "    \n",
    "split_stratified_labels = []\n",
    "for key in list(np.unique(input_feats.loc[:, 'subject'])):\n",
    "    split_stratified_labels.append(labels[key].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13de6b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since if we are predicting the age as a task we don't use it as an input feature\n",
    "if age:\n",
    "    input_feats = input_feats.drop(columns=['visit_age'])\n",
    "    \n",
    "# We are doing 5-fold cross validation se we are creating dictionaries to save the data for each fold\n",
    "partition = {}\n",
    "folds = {}\n",
    "subject_folds = {}\n",
    "mfb_folds = {}\n",
    "folds_of_the_labels = {}\n",
    "folds_of_the_ages = {}\n",
    "counter = 0\n",
    "\n",
    "# We are creating the object scaler to normalize our input data from -1 to 1\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "# We are performing stratified CV since we hace class imbalance\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=False)  # False for reproducible folds\n",
    "X = np.array(list(np.unique(input_feats.loc[:, 'subject'])))\n",
    "y = np.array(split_stratified_labels)\n",
    "kf.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ce6efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to separate the data to train and test sets for each fold, normalize them and also \n",
    "# exclude the 'fake control' subjects from the test set. \n",
    "# 'Fake controls' are subjects that have 'True' in any depression construct in at least one of the visits. \n",
    "# The four constructs are: 'positive_valence', 'negative_valence', 'arousal', 'cognitive' \n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    labels_fold = labels.copy()\n",
    "    ages_fold = ages.copy()\n",
    "    subject_ages = {}\n",
    "    subject_post = {}\n",
    "\n",
    "    train_subj = input_feats.loc[input_feats['subject'].isin(list(X[train_index]))]\n",
    "    test_subj = input_feats.loc[input_feats['subject'].isin(list(X[test_index]))]\n",
    "    # In this version of the dataset the 21 first columns contain features that should not be used as input \n",
    "    # for the prediction.\n",
    "    # These features are the personal information, like sex, race etc., the labels that we are predicting and other\n",
    "    # potential prediction variables.\n",
    "    X_train = train_subj.iloc[:, 21:]\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    X_train = pd.DataFrame(data=X_train, columns=train_subj.columns[21:])\n",
    "    X_train = X_train.set_index(train_subj.index)\n",
    "    X_train.insert(0, 'subject', train_subj.loc[:, 'subject'], True)\n",
    "\n",
    "    X_test = test_subj.iloc[:, 21:]\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_test = pd.DataFrame(data=X_test, columns=test_subj.columns[21:])\n",
    "    X_test = X_test.set_index(test_subj.index)\n",
    "    X_test.insert(0, 'subject', test_subj.loc[:, 'subject'], True)\n",
    "\n",
    "    partition['test'] = list()\n",
    "    partition['train'] = list()\n",
    "    for subject in input_feats.subject.unique():\n",
    "        if subject in list(X[train_index]):\n",
    "            subj_visits = X_train[X_train['subject'] == subject]\n",
    "            subject_ages[subject] = subj_visits\n",
    "            partition['train'].append(subject)\n",
    "            \n",
    "        elif subject in list(X[test_index]):\n",
    "            subj_visits = X_test[X_test['subject'] == subject]\n",
    "            subject_ages[subject] = subj_visits\n",
    "            partition['test'].append(subject)\n",
    "\n",
    "    folds[counter] = partition.copy()\n",
    "\n",
    "    # Subject-specific dataset with all the visits and post-processed for training\n",
    "    for key in list(partition['train'] + partition['test']):\n",
    "        df = subject_ages[key]\n",
    "        df = df.iloc[:, 1:]\n",
    "        subject_post[key] = df\n",
    "\n",
    "    subject_folds[counter] = subject_post.copy()\n",
    "    folds_of_the_labels[counter] = labels_fold.copy()\n",
    "    folds_of_the_ages[counter] = ages_fold.copy()\n",
    "\n",
    "    # Since we have class imbalance we are using weights in the binary cross entropy during training\n",
    "    # These weights are calculated separately for each fold since we only take the training \n",
    "    # set into account to calculate them.\n",
    "    y_train = train_subj.loc[:, construct]\n",
    "    number_neg_samples = np.sum(y_train.values == False)\n",
    "    num_pos_samples = np.sum(y_train.values == True)\n",
    "    mfb = number_neg_samples / num_pos_samples\n",
    "    mfb_folds[counter] = mfb.copy()\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25ec9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'shuffle': True,\n",
    "          'num_workers': 0,\n",
    "          'batch_size': 1} # One batch contains all the visits of each subject\n",
    "\n",
    "results = {}\n",
    "avg_results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7ea5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete for fold 0 complete.\n",
      "Model training complete for fold 1 complete.\n",
      "Model training complete for fold 2 complete.\n",
      "Model training complete for fold 3 complete.\n",
      "Model training complete for fold 4 complete.\n"
     ]
    }
   ],
   "source": [
    "# Train models for each fold\n",
    "\n",
    "for fold in folds.keys():\n",
    "\n",
    "    # Get the data per fold\n",
    "    partition = folds[fold]\n",
    "    subject_post = subject_folds[fold]\n",
    "    pos_weight = mfb_folds[fold]\n",
    "    labels_f = folds_of_the_labels[fold]\n",
    "    ages_f = folds_of_the_ages[fold]\n",
    "\n",
    "    # Dataset generators\n",
    "    training_set = Dataset(partition['train'], subject_post, labels_f, ages_f, age)\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = Dataset(partition['test'], subject_post, labels_f, ages_f, age)\n",
    "    validation_generator = torch.utils.data.DataLoader(validation_set, **params)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # The parameters I trained with, after hyperparameter tuning\n",
    "    epoch = 0\n",
    "    max_epochs = 30\n",
    "\n",
    "    feature_dim = 128\n",
    "    input_dim = next(iter(training_generator))[0].shape[2]\n",
    "    output_dim = 1\n",
    "    n_layers = 1\n",
    "    hidden_dim = 64\n",
    "\n",
    "    \n",
    "    if age:\n",
    "        model = AgeGRUNet(feature_dim=feature_dim, input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                          n_layers=n_layers, seq2seq=seq2seq, device=device)\n",
    "    else:\n",
    "        model = GRUNet(feature_dim=feature_dim, input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                          n_layers=n_layers, seq2seq=seq2seq, device=device)\n",
    "\n",
    "    # Loss chosen for binary classification task\n",
    "    score_criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).float().to(device))\n",
    "\n",
    "    if age:\n",
    "        # Loss chosen for age prediction\n",
    "        age_criterion = nn.MSELoss()\n",
    "        criterion = {}\n",
    "        criterion['score'] = score_criterion\n",
    "        criterion['age'] = age_criterion\n",
    "    else:\n",
    "        criterion = score_criterion\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if age:\n",
    "        model_trained, h = train_gru_age(model=model, criterion=criterion, optimizer=optimizer, max_epochs=max_epochs,\n",
    "                                         train_loader=training_generator, val_loader=validation_generator,\n",
    "                                         device=device, seq2seq=seq2seq, params=params)\n",
    "    else:\n",
    "        model_trained, h = train_gru(model=model, criterion=criterion, optimizer=optimizer, max_epochs=max_epochs,\n",
    "                                         train_loader=training_generator, val_loader=validation_generator,\n",
    "                                         device=device, seq2seq=seq2seq, params=params)\n",
    "    \n",
    "    # Path to save the models\n",
    "    if age:\n",
    "        output_path = f'{construct}_fold_{fold}_tabular_no_aces.ckpt'\n",
    "    else:\n",
    "        output_path = f'{construct}_fold_{fold}_tabular_no_aces_no_age.ckpt'\n",
    "    \n",
    "    print(f'Model training complete for fold {fold} complete.')\n",
    "    # Uncomment to save models\n",
    "    #torch.save(model_trained.state_dict(), output_path)\n",
    "    #print(f'Model Saved at: {output_path}')\n",
    "    \n",
    "    if seq2seq:\n",
    "        if age:\n",
    "            results[f'split{fold}'] = evaluate_all_timesteps_age_per_subject(model=model_trained, val_loader=validation_generator,\n",
    "                                                                 hidden=h, device=device)\n",
    "        else:\n",
    "            results[f'split{fold}'] = evaluate_all_timesteps_per_subject(model=model_trained, val_loader=validation_generator, hidden=h, device=device)\n",
    "    else:\n",
    "        results[f'split{fold}'] = evaluate_last_timestep(model=model_trained, val_loader=validation_generator, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c17ddf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average results for negative_valence:\n",
      "{'subject_accuracy': 0.8743225806451613, 'subject_macro_accuracy': 0.7455922744294735, 'accuracy': 0.8743225806451613, 'macro_accuracy': 0.7455922744294735, 'f1-score': 0.6481935022869874}\n"
     ]
    }
   ],
   "source": [
    "# Average the results over the 5 folds and print the metrics\n",
    "avg_acc = 0.0\n",
    "avg_bacc = 0.0\n",
    "avg_f1 = 0.0\n",
    "subj_acc = 0.0\n",
    "subj_macro_acc = 0.0\n",
    "\n",
    "for key in results.keys():\n",
    "    subj_macro_acc += results[key]['subject_macro_accuracy']\n",
    "    subj_acc += results[key]['subject_accuracy']\n",
    "    avg_acc += results[key]['accuracy']\n",
    "    avg_bacc += results[key]['balanced_accuracy']\n",
    "    avg_f1 += results[key]['f1-score']\n",
    "\n",
    "avg_results_dict['subject_accuracy'] = subj_acc / len(folds.keys())\n",
    "avg_results_dict['subject_macro_accuracy'] = subj_macro_acc / len(folds.keys())\n",
    "avg_results_dict['accuracy'] = avg_acc / len(folds.keys())\n",
    "avg_results_dict['macro_accuracy'] = avg_bacc / len(folds.keys())\n",
    "avg_results_dict['f1-score'] = avg_f1 / len(folds.keys())\n",
    "print(f'Average results for {construct}:')\n",
    "print(avg_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2901d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification",
   "language": "python",
   "name": "classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
